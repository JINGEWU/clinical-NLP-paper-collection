# Paper collection

## Model



### PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering [[paper]](https://arxiv.org/pdf/2305.10415.pdf) [[repo]](https://xiaoman-zhang.github.io/PMC-VQA/)

- Propose a model: MedVInT, and VQA benchmark data: PMC-VQA

<img src="/Users/jingewu/Library/Application Support/typora-user-images/image-20231105202214022.png" alt="image-20231105202214022" style="zoom:75%;" />

### LLaVA: Visual Instruction Tuning [[paper]](https://arxiv.org/pdf/2304.08485.pdf) [[repo]](https://llava-vl.github.io/)

![image-20231106132624401](/Users/jingewu/Library/Application Support/typora-user-images/image-20231106132624401.png)

### LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day [[paper]](https://arxiv.org/pdf/2306.00890.pdf) [[model]](https://aka.ms/llava-med)

![image-20231106131444267](/Users/jingewu/Library/Application Support/typora-user-images/image-20231106131444267.png)

### CLIP: Learning Transferable Visual Models From Natural Language Supervision [[paper]](https://arxiv.org/pdf/2103.00020.pdf) [[repo]](https://github.com/OpenAI/CLIP) 

### MedCLIP: Contrastive Learning from Unpaired Medical Images and Text [[paper]](https://arxiv.org/pdf/2210.10163.pdf) [[repo]](https://github.com/RyanWangZf/MedCLIP)

### Q2ATransformer: Improving Medical VQA via an Answer Querying Decoder [[paper]](https://arxiv.org/pdf/2304.01611.pdf)

### Flamingo: a Visual Language Model for Few-Shot Learning [[paper]](https://arxiv.org/pdf/2204.14198.pdf) 

### OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models [[paper]](https://arxiv.org/pdf/2308.01390.pdf) [[repo]](https://github.com/mlfoundations/open_flamingo) [[huggingface]](https://huggingface.co/openflamingo/OpenFlamingo-3B-vitl-mpt1b)

### InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning [[paper]](https://arxiv.org/pdf/2305.06500.pdf) [[repo]](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)

![image-20231106133912000](/Users/jingewu/Library/Application Support/typora-user-images/image-20231106133912000.png)

### PubMedCLIP: How Much Does CLIP Benefit Visual Question Answering in the Medical Domain? [[paper]](https://aclanthology.org/2023.findings-eacl.88.pdf) [[repo]](https://github.com/sarahESL/PubMedCLIP)

### JPG - Jointly Learn to Align: Automated Disease Prediction and Radiology Report Generation [[paper]](https://aclanthology.org/2022.coling-1.523.pdf)

![image-20231106132251190](/Users/jingewu/Library/Application Support/typora-user-images/image-20231106132251190.png)

### PromptMRG: Diagnosis-Driven Prompts for Medical Report Generation [[paper]](https://arxiv.org/pdf/2308.12604.pdf)

![image-20231106114134948](/Users/jingewu/Library/Application Support/typora-user-images/image-20231106114134948.png)

### DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task [[paper]](https://arxiv.org/pdf/2304.01097.pdf) [[repo]](https://github.com/xionghonglin/DoctorGLM)

![image-20231106115302884](/Users/jingewu/Library/Application Support/typora-user-images/image-20231106115302884.png)

### XrayGPT: Chest Radiographs Summarization using Large Medical Vision-Language Models [[paper]](https://arxiv.org/pdf/2306.07971.pdf) [[repo]](https://github.com/mbzuai-oryx/XrayGPT)

![image-20231106115439784](/Users/jingewu/Library/Application Support/typora-user-images/image-20231106115439784.png)

### Radiology-GPT: A Large Language Model for Radiology [[paper]](https://arxiv.org/pdf/2306.08666.pdf) [[huggingface]](https://huggingface.co/spaces/allen-eric/radiology-gpt)

![image-20231106115623816](/Users/jingewu/Library/Application Support/typora-user-images/image-20231106115623816.png)



### MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models [[paper]](https://arxiv.org/abs/2304.10592) [[web]](https://minigpt-4.github.io/)

![image-20231106131953723](/Users/jingewu/Library/Application Support/typora-user-images/image-20231106131953723.png)

### MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning [[paper]](https://minigpt-v2.github.io/)

- LLaMA2 + ViT with multi-task instruction tuning

  ![image-20231106120238826](/Users/jingewu/Library/Application Support/typora-user-images/image-20231106120238826.png)

### RWKV: Reinventing RNNs for the Transformer Era [[paper]](https://arxiv.org/abs/2305.13048) [[web]](https://wiki.rwkv.com/)

- RNN with Transformer

### M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization [[paper]](https://arxiv.org/pdf/2307.08347.pdf) [[repo]](https://github.com/cheliu-computation/M-FLAG-MICCAI2023)

### Vision-Language Modelling for Radiological Imaging and Reports in the Low Data Regime [[paper]](https://arxiv.org/pdf/2303.17644.pdf) 

### CAMANet: Class Activation Map Guided Attention Network for Radiology Report Generation [[paper]](https://arxiv.org/pdf/2211.01412.pdf)

### KiUT: Knowledge-injected U-Transformer for Radiology Report Generation [[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_KiUT_Knowledge-Injected_U-Transformer_for_Radiology_Report_Generation_CVPR_2023_paper.pdf)

![image-20231106121143309](/Users/jingewu/Library/Application Support/typora-user-images/image-20231106121143309.png)

### Replace and Report: NLP Assisted Radiology Report Generation [[paper]](https://arxiv.org/pdf/2306.17180.pdf)

![image-20231106121325197](/Users/jingewu/Library/Application Support/typora-user-images/image-20231106121325197.png)

### ChatRadio-Valuer: A Chat Large Language Model for Generalizable Radiology Report Generation Based on Multi-institution and Multi-system Data [[paper]](https://arxiv.org/pdf/2310.05242.pdf)

![image-20231106121512314](/Users/jingewu/Library/Application Support/typora-user-images/image-20231106121512314.png)

### K-PathVQA: Knowledge-Aware Multimodal Representation for Pathology Visual Question Answering [[paper]](https://ieeexplore.ieee.org/abstract/document/10177927?casa_token=_22BcWB2WNIAAAAA:XXGST5ZAg_pfDSlx9PwDWZd1uFBmxd8PCIBFmtDHv38byw7fuTNI4CBrtA5XL6nIDYtw78Etwg)



### What Matters in Training a GPT4-Style Language Model with Multimodal Inputs? [[paper]](https://arxiv.org/pdf/2307.02469.pdf)

![image-20231106133415105](/Users/jingewu/Library/Application Support/typora-user-images/image-20231106133415105.png)



### MedAlpaca: [[repo]](https://github.com/kbressem/medAlpaca)

### PMC-LLaMA: Towards Building Open-source Language Models for Medicine [[paper]](https://arxiv.org/abs/2304.14454) [[repo]](https://github.com/)





## multi-view:

### MVCO-DOT: MULTI-VIEW CONTRASTIVE DOMAIN TRANSFER NETWORK FOR MEDICAL REPORT GENERATION [[paper]](https://arxiv.org/pdf/2304.07465.pdf)

### IIHT: Medical Report Generation with Image-to-Indicator Hierarchical Transformer [[paper]](https://arxiv.org/pdf/2308.05633.pdf)

![image-20231106121419888](/Users/jingewu/Library/Application Support/typora-user-images/image-20231106121419888.png)

### C 2M-DoT: Cross-modal consistent multi-view medical report generation with domain transfer network [[paper]](https://arxiv.org/pdf/2310.05355.pdf)























## Good survey papers:

FOUNDATIONAL MODELS IN MEDICAL IMAGING: A COMPREHENSIVE SURVEY AND FUTURE VISION [[paper]]((https://arxiv.org/pdf/2310.18689.pdf))

![image-20231106113050961](/Users/jingewu/Library/Application Support/typora-user-images/image-20231106113050961.png)

Instruction Tuning for Large Language Models: A Survey [[paper]](https://arxiv.org/pdf/2308.10792.pdf)



Multimodal Foundation Models: From Specialists to General-Purpose Assistants [[paper]](https://arxiv.org/pdf/2309.10020.pdf) 

![image-20231106120400698](/Users/jingewu/Library/Application Support/typora-user-images/image-20231106120400698.png)



















## Evaluation

Multimodal ChatGPT for Medical Applications: an Experimental Study of GPT-4V [[paper]](https://arxiv.org/pdf/2310.19061.pdf) [[repo]](https://github.com/ZhilingYan/GPT4V-Medical-Report)

- Evaluation GPT4v for medical domain 

MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models [[paper]](https://arxiv.org/pdf/2306.13394.pdf)

Foundation Metrics: Quantifying Effectiveness of Healthcare Conversations powered by Generative AI [[paper]](https://arxiv.org/pdf/2309.12444.pdf)

![image-20231106115912329](/Users/jingewu/Library/Application Support/typora-user-images/image-20231106115912329.png)

Evaluating large language models for use in healthcare: A framework for translational value assessment [[paper]](https://www.sciencedirect.com/science/article/pii/S2352914823001508)

![image-20231106115825328](/Users/jingewu/Library/Application Support/typora-user-images/image-20231106115825328.png)

Faithful AI in Medicine: A Systematic Review with Large Language Models and Beyond [[paper]](https://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC10312867&blobtype=pdf)

![image-20231106132809840](/Users/jingewu/Library/Application Support/typora-user-images/image-20231106132809840.png)

### Towards Generalist Foundation Model for Radiology by Leveraging Web-scale 2D&3D Medical Data [[paper]](https://arxiv.org/pdf/2308.02463.pdf)

![image-20231106133103185](/Users/jingewu/Library/Application Support/typora-user-images/image-20231106133103185.png)

### Towards Generalist Biomedical AI [[paper]](https://arxiv.org/pdf/2307.14334.pdf)

![image-20231106133023278](/Users/jingewu/Library/Application Support/typora-user-images/image-20231106133023278.png)



Hallucination: 

HALTT4LLM - Hallucination Trivia Test for Large Language Models [[Link]](https://github.com/manyoso/haltt4llm#haltt4llm---hallucination-trivia-test-for-large-language-models)

HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models [[paper]](https://arxiv.org/pdf/2305.11747.pdf)

Object Hallucination in Image Captioning [[Link]](https://arxiv.org/pdf/1809.02156.pdf)

- CHAIR

Survey of Hallucination in Natural Language Generation [[paper]](https://dl.acm.org/doi/10.1145/3571730)

Med-HALT: Medical Domain Hallucination Test for Large Language Models [[paper]](https://arxiv.org/pdf/2307.15343.pdf)



## useful repo:

Model:

Awesome-LLM [[Link]](https://github.com/Hannibal046/Awesome-LLM)

awesome-multimodal-in-medical-imaging [[Link]](https://github.com/richard-peng-xia/awesome-multimodal-in-medical-imaging)

Awesome-Foundation-Models [[Link]](https://github.com/uncbiag/Awesome-Foundation-Models)

PluginGPT [[Link]](https://github.com/HUANGLIZI/PluginGPT)

Awesome-Multimodal-Large-Language-Models [[Link]](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)

Evaluation:

Awesome-Multimodal-Large-Language-Models [[Link]](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)

Awesome-LLMs-Evaluation-Papers [[Link]](https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers)

